{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e9f413",
   "metadata": {},
   "source": [
    " # 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2200981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>刚刚，欧盟以439票赞成，104票反对，88票弃权通过一项决议，承认胡安?瓜伊多为委内瑞拉合...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>顺其自然，不是代表我们可以不努力，而是努力之后有勇气接受成败 ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#金华[超话]#感谢，在这座城市遇见你！ http://t.cn/EtQuI71 ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A股那么多雷，你为啥不说的延展思考 ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018年穷的吃土，看看2019年怎么样！大家来试试自己的吧！ ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>【年画话新年 台州春节看展去】春节期间，我市什么地方能看热闹的大戏，什么地方能安静地看书画展...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>苹果 2019：放弃 5G，开放内容http://t.cn/EtR4YJ6 ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>早盘的回压对于分时的反弹是有利的，今天的安慰红应该能够保持 ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>好可爱啊！！自从流浪喵咪被收养后，逐渐掌握了各种要零食的技能，从此猫生逆袭~主人和喵子一起演...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1977年林青霞和胡因梦同游意大利，这打扮就是放今天也是潮的不行！ ????今年春节去意大利...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    刚刚，欧盟以439票赞成，104票反对，88票弃权通过一项决议，承认胡安?瓜伊多为委内瑞拉合...\n",
       "1                    顺其自然，不是代表我们可以不努力，而是努力之后有勇气接受成败 ??\n",
       "2          #金华[超话]#感谢，在这座城市遇见你！ http://t.cn/EtQuI71 ??\n",
       "3                                 A股那么多雷，你为啥不说的延展思考 ??\n",
       "4                   2018年穷的吃土，看看2019年怎么样！大家来试试自己的吧！ ??\n",
       "..                                                 ...\n",
       "195  【年画话新年 台州春节看展去】春节期间，我市什么地方能看热闹的大戏，什么地方能安静地看书画展...\n",
       "196           苹果 2019：放弃 5G，开放内容http://t.cn/EtR4YJ6 ??\n",
       "197                   早盘的回压对于分时的反弹是有利的，今天的安慰红应该能够保持 ??\n",
       "198  好可爱啊！！自从流浪喵咪被收养后，逐渐掌握了各种要零食的技能，从此猫生逆袭~主人和喵子一起演...\n",
       "199  1977年林青霞和胡因梦同游意大利，这打扮就是放今天也是潮的不行！ ????今年春节去意大利...\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "from snownlp import sentiment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba as jie #导入结巴分词，需要自行下载安装 # version 0.42.1\n",
    "import gensim as gen  # version 4.2.0\n",
    "import wordcloud\n",
    "\n",
    "path='D:\\\\01study\\\\01 大三\\\\python\\\\大作业\\\\数据及说明\\\\原微博样本.csv'\n",
    "data = pd.read_csv(path, encoding='utf-8',  low_memory=False)\n",
    "# l1 = len(data)\n",
    "data = pd.DataFrame(data['content'].unique())\n",
    "# l2 = len(data)\n",
    "#data = data.iloc[:,1:2].unique()\n",
    "data.to_csv('outputfile.txt', index=False, header=False, encoding='utf-8')\n",
    "data.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b88037",
   "metadata": {},
   "source": [
    "# 内容情绪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a90da968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>顺其自然，不是代表我们可以不努力，而是努力之后有勇气接受成败 ??</td>\n",
       "      <td>0.983375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#金华[超话]#感谢，在这座城市遇见你！ http://t.cn/EtQuI71 ??</td>\n",
       "      <td>0.866584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018年穷的吃土，看看2019年怎么样！大家来试试自己的吧！ ??</td>\n",
       "      <td>0.711233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>如何做一个体面的穷人[笑cry] ??</td>\n",
       "      <td>0.956669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>每一个清晨，都是一个希望，都有一个梦想！不管昨天怎样低落， 总会看见今天太阳的升起； 不管昨...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>年越来越近，心茫然不安，一年又一年从指间悄然流失，增长的只有年龄，不知从何时开始，过年不再是...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>不要因为寂寞而错爱，更别因为错爱而寂寞一生。 ??</td>\n",
       "      <td>0.999911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>这小子变化有点666，从一块竹炭变成了白胖小鲜肉[色][色][色]我的大宝#爱抽风的老陈头#...</td>\n",
       "      <td>0.991533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>啊这。。。。。[悲伤][悲伤][悲伤]不会吧我分享了 @育儿心语521 的头条文章 http...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>发布了头条文章：《20190201节前金融规则大修改》 我自己重仓券商股希望是利好的，但平心...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>曾以为我配不上你…事实上，是你不值得拥有我 ??</td>\n",
       "      <td>0.87652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>其实，人生有许多无声的感动，花儿，静静地盛开，不为谁来，只为彰显自己的精彩！小草，默默的生长...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>见证你一路成长，期待你越来越棒！永远永远支持#孟瑞#  #孟瑞能量月# http://t.c...</td>\n",
       "      <td>0.999976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>有些人一直没有机会见，等有机会见了，却又犹豫了，相见不如不见。有些事一直没有机会做，等有机会...</td>\n",
       "      <td>0.99996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>在苹果公布了第一季2019年第一季财报中，iPhone的营收则是下跌了15%，显示了中国市场...</td>\n",
       "      <td>0.739134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>人生的磨难是很多的，所以我们不可对于每一件轻微的伤害都过于敏感。在生活磨难面前，精神上的坚强...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>#新浪看点# 香飘飘砸五千万开奶茶店自救，线下网红店真能救香飘飘吗？http://t.cn/...</td>\n",
       "      <td>0.981751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>无法避开挑衅和挑战的话，只有赢战了。分享单曲http://t.cn/Et8Y8gS（@网易云...</td>\n",
       "      <td>0.998534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>己亥年，木为太岁，命宫射手，命宫双鱼，日柱甲木，乙木，桃花运up，up，up，戊戌年年底的金...</td>\n",
       "      <td>0.998355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0          1\n",
       "0                                                   0  sentiment\n",
       "1                   顺其自然，不是代表我们可以不努力，而是努力之后有勇气接受成败 ??   0.983375\n",
       "2         #金华[超话]#感谢，在这座城市遇见你！ http://t.cn/EtQuI71 ??   0.866584\n",
       "3                  2018年穷的吃土，看看2019年怎么样！大家来试试自己的吧！ ??   0.711233\n",
       "4                                 如何做一个体面的穷人[笑cry] ??   0.956669\n",
       "5   每一个清晨，都是一个希望，都有一个梦想！不管昨天怎样低落， 总会看见今天太阳的升起； 不管昨...        1.0\n",
       "6   年越来越近，心茫然不安，一年又一年从指间悄然流失，增长的只有年龄，不知从何时开始，过年不再是...        1.0\n",
       "7                           不要因为寂寞而错爱，更别因为错爱而寂寞一生。 ??   0.999911\n",
       "8   这小子变化有点666，从一块竹炭变成了白胖小鲜肉[色][色][色]我的大宝#爱抽风的老陈头#...   0.991533\n",
       "9   啊这。。。。。[悲伤][悲伤][悲伤]不会吧我分享了 @育儿心语521 的头条文章 http...        1.0\n",
       "10  发布了头条文章：《20190201节前金融规则大修改》 我自己重仓券商股希望是利好的，但平心...        1.0\n",
       "11                           曾以为我配不上你…事实上，是你不值得拥有我 ??    0.87652\n",
       "12  其实，人生有许多无声的感动，花儿，静静地盛开，不为谁来，只为彰显自己的精彩！小草，默默的生长...        1.0\n",
       "13  见证你一路成长，期待你越来越棒！永远永远支持#孟瑞#  #孟瑞能量月# http://t.c...   0.999976\n",
       "14  有些人一直没有机会见，等有机会见了，却又犹豫了，相见不如不见。有些事一直没有机会做，等有机会...    0.99996\n",
       "15  在苹果公布了第一季2019年第一季财报中，iPhone的营收则是下跌了15%，显示了中国市场...   0.739134\n",
       "16  人生的磨难是很多的，所以我们不可对于每一件轻微的伤害都过于敏感。在生活磨难面前，精神上的坚强...        1.0\n",
       "17  #新浪看点# 香飘飘砸五千万开奶茶店自救，线下网红店真能救香飘飘吗？http://t.cn/...   0.981751\n",
       "18  无法避开挑衅和挑战的话，只有赢战了。分享单曲http://t.cn/Et8Y8gS（@网易云...   0.998534\n",
       "19  己亥年，木为太岁，命宫射手，命宫双鱼，日柱甲木，乙木，桃花运up，up，up，戊戌年年底的金...   0.998355"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentslist = []\n",
    "pos = ['0']\n",
    "neg = ['0']\n",
    "pos_sentiment = ['sentiment']\n",
    "neg_sentiment = ['sentiment']\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    s = SnowNLP(str(data[0][i]))\n",
    "    sentiment = s.sentiments\n",
    "    sentimentslist.append(sentiment)\n",
    "    if sentiment >= 0.6:\n",
    "        pos.append(data[0][i])\n",
    "        pos_sentiment.append(sentiment)\n",
    "    if sentiment <= 0.4:\n",
    "        neg.append(data[0][i])\n",
    "        neg_sentiment.append(sentiment)       \n",
    "\n",
    "d1=[pos,pos_sentiment]\n",
    "d1=pd.DataFrame(d1)\n",
    "d1=np.transpose(d1)\n",
    "d1.to_csv(\"process_end_正面情感结果.txt\",index=False,header=False,encoding='utf-8')\n",
    "\n",
    "d2=[neg,neg_sentiment]\n",
    "d2=pd.DataFrame(d2)\n",
    "d2=np.transpose(d2)\n",
    "d2.to_csv(\"process_end_负面情感结果.txt\",index=False,header=False,encoding='utf-8')\n",
    "d1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928f583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b2ec62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\last\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yun\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.669 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                         0\n",
       "1         顺其自然 ， 不是 代表 我们 可以 不 努力 ， 而是 努力 之后 有 勇气 接受 成败 ...\n",
       "2         # 金华 [ 超话 ] # 感谢 ， 在 这座 城市 遇见 你 ！   http : / /...\n",
       "3         2018 年 穷 的 吃 土 ， 看看 2019 年 怎么样 ！ 大家 来 试试 自己 的 ...\n",
       "4                           如何 做 一个 体面 的 穷人 [ 笑 cry ]   ? ?\n",
       "                                ...                        \n",
       "412980    猛涨 ！ 几乎 家家 有 ， 已涨 1000 元 http : / / t . cn / A...\n",
       "412981    # 1027 交通 回顾 # 今天 是 周日 ， 高峰 时段 车流量 也 不大 ， 全市 交...\n",
       "412982    【 # 2020 年 GDP 十强 城市 #   ： 广州 险胜 重庆 ， 南京 首次 入榜...\n",
       "412983                  # 家属 证实 学车 后 失联 大学生 遇害 # 可惜 了   ? ?\n",
       "412984                              粤海 街道 的 雏形 [ 嘻嘻 ]   ? ?\n",
       "Name: 0, Length: 412985, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba \n",
    "\n",
    "#参数初始化\n",
    "inputfile1 = 'process_end_正面情感结果.txt'\n",
    "inputfile2 = 'process_end_负面情感结果.txt'\n",
    "outputfile1 = 'neg_cut.txt'\n",
    "outputfile2 = 'pos_cut.txt'\n",
    "\n",
    "data1 = pd.read_csv(inputfile1, encoding = 'utf-8', header = None) #读入数据\n",
    "data2 = pd.read_csv(inputfile2, encoding = 'utf-8', header = None)\n",
    "\n",
    "mycut = lambda s: ' '.join(jieba.cut(s)) #自定义简单分词函数,先识别句子中的中文单词，然后把中文单词通过空格连接起来\n",
    "#上面一句代码中，s是入口参数，.join前面的空格表示把jieba库处理过后的s中的词语jieba.cut(s)，用空格来连接。\n",
    "\n",
    "data1 = data1[0].apply(mycut) #通过“广播”形式分词，加快速度。\n",
    "data2 = data2[0].apply(mycut)\n",
    "\n",
    "data1.to_csv(outputfile1, index = False, header = False, encoding = 'utf-8') #保存结果\n",
    "data2.to_csv(outputfile2, index = False, header = False, encoding = 'utf-8')\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2fad4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     [0/x]\n",
       "1         [顺其自然/i,  /x, ，/x,  /x, 不是/c,  /x, 代表/n,  /x, ...\n",
       "2         [#/x,  /x, 金华/nz,  /x, [/x,  /x, 超话/v,  /x, ]/...\n",
       "3         [2018/m,  /x, 年/m,  /x, 穷/a,  /x, 的/uj,  /x, 吃...\n",
       "4         [如何/r,  /x, 做/v,  /x, 一个/m,  /x, 体面/n,  /x, 的/...\n",
       "                                ...                        \n",
       "412980    [猛涨/v,  /x, ！/x,  /x, 几乎/d,  /x, 家家/n,  /x, 有/...\n",
       "412981    [#/x,  /x, 1027/m,  /x, 交通/n,  /x, 回顾/v,  /x, ...\n",
       "412982    [【/x,  /x, #/x,  /x, 2020/m,  /x, 年/m,  /x, GD...\n",
       "412983    [#/x,  /x, 家属/n,  /x, 证实/n,  /x, 学车/n,  /x, 后/...\n",
       "412984    [粤海/ns,  /x, 街道/n,  /x, 的/uj,  /x, 雏形/n,  /x, ...\n",
       "Name: 0, Length: 412985, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 词性标注\n",
    "import jieba.posseg as psg\n",
    "#参数初始化\n",
    "inputfile1 = 'neg_cut.txt'\n",
    "inputfile2 = 'pos_cut.txt'\n",
    "outputfile1 = 'neg_psg.txt'\n",
    "outputfile2 = 'pos_psg.txt'\n",
    "\n",
    "data1 = pd.read_csv(inputfile1, encoding = 'utf-8', header = None) #读入数据\n",
    "data2 = pd.read_csv(inputfile2, encoding = 'utf-8', header = None)\n",
    "\n",
    "\n",
    "def mycut(s):\n",
    "    seg = psg.cut(s)\n",
    "    words=[]\n",
    "    for w in seg:\n",
    "        words.append(w)\n",
    "    return words\n",
    "\n",
    "data1 = data1[0].apply(mycut) \n",
    "data2 = data2[0].apply(mycut)\n",
    "\n",
    "data1.to_csv(outputfile1, index = False, header = False, encoding = 'utf-8') #保存结果\n",
    "data2.to_csv(outputfile2, index = False, header = False, encoding = 'utf-8')\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22cd4bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  []\n",
       "1          [努力, 顺其自然, 成败, 勇气, 接受, 而是, 代表, 之后, 不是, 可以]\n",
       "2         [超话, http, cn, EtQuI71, 金华, 遇见, 感谢, 这座, 城市]\n",
       "3                   [2018, 2019, 试试, 怎么样, 看看, 大家, 自己]\n",
       "4                               [cry, 体面, 穷人, 如何, 一个]\n",
       "                             ...                     \n",
       "412980     [已涨, 1000, http, cn, A65C3zLf, 猛涨, 家家, 几乎]\n",
       "412981    [山路, 浣纱, 排行, 西路, 延安, 1027, 湿滑, 交通, 车流量, 晚安]\n",
       "412982    [GDP, 2020, 险胜, 前十, 重庆, 广州, 南京, 首次, 城市, 苏州]\n",
       "412983                  [学车, 失联, 遇害, 家属, 大学生, 可惜, 证实]\n",
       "412984                               [粤海, 嘻嘻, 雏形, 街道]\n",
       "Name: 0, Length: 412985, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 提取关键词\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "#参数初始化\n",
    "inputfile1 = 'neg_cut.txt'\n",
    "inputfile2 = 'pos_cut.txt'\n",
    "outputfile1 = 'neg_keywords.txt'\n",
    "outputfile2 = 'pos_keywords.txt'\n",
    "\n",
    "data1 = pd.read_csv(inputfile1, encoding = 'utf-8', header = None) #读入数据\n",
    "data2 = pd.read_csv(inputfile2, encoding = 'utf-8', header = None)\n",
    "\n",
    "def mycut(s):\n",
    "    keywords = jieba.analyse.extract_tags(s,topK=10, withWeight= False, allowPOS=()) #TF-IDF\n",
    "    #keywords = jieba.analyse.textrank(s,topK=10, withWeight= False, allowPOS=('ns','n','vn','v')) #TextRank\n",
    "    return keywords\n",
    "\n",
    "data1 = data1[0].apply(mycut) #通过“广播”形式分词，加快速度。\n",
    "data2 = data2[0].apply(mycut)\n",
    "\n",
    "data1.to_csv(outputfile1, index = False, header = False, encoding = 'utf-8') #保存结果\n",
    "data2.to_csv(outputfile2, index = False, header = False, encoding = 'utf-8')\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "729a9f33",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32488/3012631758.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# 负面主题分析\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mneg_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 建立词典\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mneg_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mneg_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mneg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# 建立语料库\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mneg_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneg_dict\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# LDA模型训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32488/3012631758.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# 负面主题分析\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mneg_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 建立词典\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mneg_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mneg_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mneg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# 建立语料库\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mneg_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneg_dict\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# LDA模型训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from wordcloud import WordCloud\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "\n",
    "# 参数初始化\n",
    "negfile = 'neg_cut.txt'\n",
    "posfile = 'pos_cut.txt'\n",
    "stoplist = 'stopwords.txt'\n",
    "\n",
    "neg = pd.read_csv(negfile, encoding='utf-8', header=None)  # 读入数据\n",
    "pos = pd.read_csv(posfile, encoding='utf-8', header=None)\n",
    "stop = pd.read_csv(stoplist, encoding='utf-8', header=None, sep='tipdm', engine='python')\n",
    "\n",
    "stop = [' ', ''] + list(stop[0])  # Pandas自动过滤了空格符，这里手动添加（在每条数据的开头加个空格）\n",
    "\n",
    "neg[1] = neg[0].apply(lambda s: s.split(' '))  # 定义一个分割函数，然后用apply广播\n",
    "neg[2] = neg[1].apply(lambda x: [i for i in x if i not in stop])  # 逐词判断是否停用词，思路同上\n",
    "\n",
    "pos[1] = pos[0].apply(lambda s: s.split(' '))\n",
    "pos[2] = pos[1].apply(lambda x: [i for i in x if i not in stop])\n",
    "\n",
    "# 负面主题分析\n",
    "neg_dict = corpora.Dictionary(neg[2])  # 建立词典\n",
    "neg_corpus = [neg_dict.doc2bow(i) for i in neg[2]]  # 建立语料库\n",
    "neg_lda = models.LdaModel(neg_corpus, num_topics=5, id2word=neg_dict)  # LDA模型训练\n",
    "for i in range(5):\n",
    "    print(neg_lda.print_topic(i))  # 输出每个主题\n",
    "    \n",
    "print(\"====================================================================\")\n",
    "# 正面主题分析\n",
    "pos_dict = corpora.Dictionary(pos[2])\n",
    "pos_corpus = [pos_dict.doc2bow(i) for i in pos[2]]\n",
    "pos_lda = models.LdaModel(pos_corpus, num_topics=5, id2word=pos_dict)\n",
    "for i in range(5):\n",
    "    print(pos_lda.print_topic(i))  # 输出每个主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "cloud_source = \" \".join(neg[2].apply(lambda x: \" \".join(x)))\n",
    "font_path = './word/simhei.ttf' #使用中文字体文件\n",
    "wc = WordCloud(background_color='white', width=500, height=350, max_font_size=80, min_font_size=4, mode='RGBA',font_path=font_path)\n",
    "wc.generate(cloud_source)\n",
    "plt.figure('negative')\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# generate negative wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143be439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "cloud_source = \" \".join(pos[2].apply(lambda x: \" \".join(x)))\n",
    "#font_path = '‪C:\\Windows\\Fonts\\simhei.ttf'\n",
    "font_path = './word/simhei.ttf'\n",
    "wc = WordCloud(background_color='white', width=500, height=350, max_font_size=80, min_font_size=4, mode='RGBA', font_path=font_path)\n",
    "wc.generate(cloud_source)\n",
    "plt.figure('negative')\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# generate positive wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae5de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
